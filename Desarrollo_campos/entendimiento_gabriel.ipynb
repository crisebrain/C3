{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correr aquí\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import spaghetti as sgt\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar los corpus\n",
    "```python \n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "### archivos contenidos en carpeta\n",
    "- spaghetti.py\n",
    "- facturaskeys.json\n",
    "- entendimiento.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definicion de palabras asociadas a los campos y de patrones de expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Prefijo': ['prefijo', 'serie'], 'NoDocumento': ['documento', 'documentos', 'nota', 'notas', 'credito', 'creditos', 'facturas', 'factura', 'numero'], 'Folio': ['folio', 'folios'], 'Inicio': ['inicial', 'inicio', 'comienza', 'empieza', 'principio', 'arranque', 'inicia', 'inicien', 'inician']}\n"
     ]
    }
   ],
   "source": [
    "dictfacturas = json.load(open(\"facturaskeys.json\"))\n",
    "print(dictfacturas)\n",
    "\n",
    "patterns = dict(Cuenta=r\"\\b[A-Za-z]{3}\\d{3}\\b\",\n",
    "                Prefijo=r\"\\b[1-9a-zA-Z]\\w{0,3}\\b\",  # wvect\n",
    "                NoDocumento=r\"\\b[0-9a-zA-Z\\-]{1,40}\\b\",  # w2vect\n",
    "                NitAdquirienteMex=r\"\\b[A-Za-z]{4}\\d{6}[A-Za-z0-9]{3}\\b\",\n",
    "                Inicio=r\".+\",\n",
    "                Folio=r\"\\d{1,16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA: \n",
    "\n",
    "    Añadir al archivo facturaskeys.json los sinónimos que crean necesarios para cada campo, \n",
    "    respetando la estructura json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexextractor(expression, field):\n",
    "    pattern = patterns[field]\n",
    "    result = re.search(pattern=pattern, string=expression)\n",
    "    if result:\n",
    "        return result.group()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def do_tagging(exp, field):\n",
    "    tokens = nltk.word_tokenize(exp)\n",
    "    tagged = sgt.pos_tag(tokens)\n",
    "    tagged = np.array([list(tup) for tup in tagged]).astype(str)\n",
    "    mask = tagged[:, 1] == 'None'\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in dictfacturas[field]:\n",
    "            tagged[i, 1] = str(field)\n",
    "    unknowns, = np.where(mask)\n",
    "    for unknown in unknowns:\n",
    "        if tagged[unknown, 0] in dictfacturas[field]:\n",
    "            tagged[unknown, 1] = field\n",
    "        else:\n",
    "            if regexextractor(tokens[unknown], field) is not None:\n",
    "                tagged[unknown, 1] = \"dato\"\n",
    "            else:\n",
    "                tagged[unknown, 1] = \"unknown\"\n",
    "    return [tuple(wordtagged) for wordtagged in tagged]\n",
    "#tag = inicio [palabras]\n",
    "\n",
    "def do_chunking(grammar, tagged, field, code):\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    chunked = cp.parse(tagged)\n",
    "    # añadir las condiciones que sean necesarias para contemplar\n",
    "    # los posibles valores\n",
    "    posibles = [\"dato\", \"Z\", \"ncfs000\", \"ncms000\", \"Fz\",\n",
    "                \"sps00\"]\n",
    "    # posibles son los tipos de palabras que pueden representar al dato\n",
    "    continuous_chunk = []\n",
    "    entity = []\n",
    "    unknowns = []\n",
    "    subt = []\n",
    "    for i, subtree in enumerate(chunked):\n",
    "        if isinstance(subtree, nltk.Tree) and subtree.label() == \"NP\":\n",
    "            # añadir las condiciones que sean necesarias para contemplar los posibles valores\n",
    "            entity += [token for token, pos in subtree.leaves()\n",
    "                       if pos in posibles]\n",
    "            unknowns += [token for token, pos in subtree.leaves()\n",
    "                         if pos == \"unknown\"]\n",
    "            subt.append(subtree)\n",
    "    if entity == []:\n",
    "        code = 0\n",
    "        if len(unknowns) > 1:\n",
    "            entity = unknowns[-1].upper()\n",
    "        elif unknowns != []:\n",
    "            entity = unknowns[0].upper()\n",
    "        else:\n",
    "            entity = None\n",
    "    elif len(entity) > 1:\n",
    "        code = 0\n",
    "        entity = entity[-1].upper()\n",
    "    else:\n",
    "        entity = entity[0].upper()\n",
    "        if regexextractor(entity, field) is not None:\n",
    "            code = 1\n",
    "        else:\n",
    "            code = 0\n",
    "    return entity, code, subt, tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quiero', 'vmip1s0'),\n",
       " ('las', 'da0fp0'),\n",
       " ('facturas', None),\n",
       " ('de', 'sps00'),\n",
       " ('hoy', 'rg'),\n",
       " ('con', 'sps00'),\n",
       " ('folio', None),\n",
       " ('de', 'sps00'),\n",
       " ('inicio', 'ncms000'),\n",
       " ('1234', None)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgt.pos_tag(\"quiero las facturas de hoy con folio de inicio 1234\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA: \n",
    "      \n",
    "      la palabra prefijo no está en el diccionario, por lo tanto como es detectada como desconocida \n",
    "      pero cumple con la expresión regular del campo, la asigna como posible dato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo de grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "grammar = r\"\"\"Q: {<dato|Z|Fz|unknown|ncfs000>}\n",
    "              T: {<dato|Fz|unknown|sps00>}\n",
    "              NP: {<Prefijo> <(vs\\w+)|(nc\\w+)|(wmi\\w+)|(spc\\w+)>* <Q>}\n",
    "              NP: {<Prefijo> <T>}\n",
    "              NP: {<Prefijo> <(vmi\\w+)|(aq\\w+)|unknown>? <sp\\w+>? <Q>}\n",
    "              NP: {<Prefijo> <dd0fs0> <vmp00sm> <sps00> <Q>}\n",
    "              NP: {<Q> <(vs\\w+)> <(da\\w+)> <Prefijo>}\n",
    "              NP: {<Q> <(p030\\w+)>? <vmip3s0>? <cs> <Prefijo>}\n",
    "            \"\"\"\n",
    "```\n",
    "### Nota:\n",
    "    \n",
    "    falta definir nodos terminales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para diseñar las reglas y probar\n",
    "\n",
    "NOTA:\n",
    "    \n",
    "    Correr cada que se cambie el grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "              NP: {<unknow> <pr\\w+> <da0ms0> <Folio> <sps00> <Inicio>}\n",
    "            \"\"\"\n",
    "\n",
    "def prueba(exp, field): \n",
    "    tagged = do_tagging(exp.lower(), field)\n",
    "    return do_chunking(grammar, tagged, field, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTA:\n",
    "    \n",
    "    salida: (VALOR, CODIGO DE VALIDEZ, FRASE ETIQUETADA QUE CUMPLE CON GRAMMAR, FRASE ETIQUETADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 0, [], [('1', 'Z'), ('12', 'Z'), ('123', 'dato'), ('99', 'Z')])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = '1 12 123 99'\n",
    "field = \"Inicio\"\n",
    "prueba(exp, field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De Gerardo\n",
    "\n",
    "Valores para agregar en facturaskeys.json\n",
    "\n",
    "```json\n",
    "{\"Estatus\": [\"estado\",\"estatus\"],\n",
    " \"Acuse\": [\"acuse\"]}\n",
    "```\n",
    "Actualizar grammar\n",
    "\n",
    "```python\n",
    "\n",
    "grammar = r\"\"\"NP: {<Estatus> <(vs\\w+)|(nc\\w+)|(wmi\\w+)|(spc\\w+)>* <dato|Z|unknown>}\n",
    "              NP: {<Estatus> <(vmi\\w+)|(aq\\w+)|unknown>? <sp\\w+>? <dato|Z|unknown>}\n",
    "              NP: {<Estatus> <dd0fs0> <vmp00sm> <sps00> <aq0cs0> <ncms000> <pr0ms000> <aq0msp> <vmip3s0> <dato|Z|unknown>}\n",
    "              NP: {<dato|Z|unknown> <(vs\\w+)> <(da\\w+)> <Estatus>}\n",
    "              NP: {<dato|Z|unknown> <(p030\\w+)>? <vmip3s0>? <cs> <Estatus>}\n",
    "           \"\"\"\n",
    "\n",
    "grammar = r\"\"\"NP: {<Acuse> <(vs\\w+)|(nc\\w+)|(wmi\\w+)|(spc\\w+)>* <dato|Z|unknown>}\n",
    "              NP: {<Acuse> <(vmi\\w+)|(aq\\w+)|unknown>? <sp\\w+>? <dato|Z|unknown>}\n",
    "              NP: {<Acuse> <dd0fs0> <vmp00sm> <sps00> <aq0cs0> <ncms000> <pr0ms000> <aq0msp> <vmip3s0> <dato|Z|unknown>}\n",
    "              NP: {<dato|Z|unknown> <(vs\\w+)> <(da\\w+)> <Acuse>}\n",
    "              NP: {<dato|Z|unknown> <(p030\\w+)>? <vmip3s0>? <cs> <Acuse>}\n",
    "           \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hola', 'saludo'), ('yo', 'pronombre')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = [(word, tag) for word, tag in zip ([\"hola\", \"yo\"], [\"saludo\", \"pronombre\"])]\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\" NP: {<saludo> <pronombre>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "chunked = cp.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [('hola', 'saludo'), ('yo', 'pronombre')])])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it i she they there we you that this and what which one is was who not\n",
      "the then had\n",
      "('amor', 'MIA')\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'count', 'index']\n",
      "['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']\n"
     ]
    }
   ],
   "source": [
    "text.similar(\"he\")\n",
    "tagged_token = nltk.tag.str2tuple(\"amor/mia\")\n",
    "print(tagged_token)\n",
    "print(dir(tagged_token))\n",
    "tupla = tuple([\"amor\",\"mia\"])\n",
    "algo = {}\n",
    "print(dir(algo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CESS-ESP and CESS-CAT TREEBANK\\n\\nThe Universitat de Barcelona (CLiC-UB), the Universidad de Alicante\\n(UA), the Universitat PolitÃšcnica de Catalunya (UPC), and the Euskal\\nHerriko Unibertsitatea (EHU-UPV) are the sole and exclusive owners of\\nthe CESS-Esp treebank.  Information for this corpus can be found at:\\nhttp://www.lsi.upc.edu/~mbertran/cess-ece2/\\n\\nThe goal of the CESS-ECE project is to create three corpora, one for\\nSpanish (CESS-ESP), one for Catalan (CESS-Cat) and one for Basque\\n(CESS-EUS), of 500,000 words for CESS-Esp and CESS-Cat and 350,000\\nwords for the CESS-Eus. These corpora will be tagged in two ways:\\nsyntactically (with constituents and functions for CESS-Esp and\\nCESS-Cat and with dependencies for CESS-Eus) and semantically (with\\nWordNet synsets). This project is based on resources from 3LB Project\\n(FIT 150500-2002-244), where 100,000 words per language were annotated\\nin the same way.\\n\\nThe version distributed with NLTK are syntactic treebanks (with\\nconstituents and functions) consisting of 1377 files for Catalan and\\n610 for Spanish. They are treebanks (with POS and lemma) with a fairly\\ncomplete tagset documented on the project\\'s website.\\n\\nA sample tree for Spanish:\\n\\n(\\n (S\\n   (snp-SUJ\\n     (espec.ms\\n       (da0ms0 El el))\\n     (grup.nom.ms\\n       (ncms000 pÃºgil pÃºgil)\\n       (s.a.ms\\n         (grup.a.ms\\n           (aq0cs0 estadounidense estadounidense)))\\n       (snp\\n         (grup.nom.ms\\n           (np0000p Will_\"Steel\"_Grigsby Will_\"Steel\"_Grigsby)))))\\n   (grup.verb\\n     (vmis3s0 conquistÃ³ conquistar))\\n   (sn-CCT\\n     (espec.fs\\n       (dd0fs0 esta este))\\n     (grup.nom.fs\\n       (ncfs000 tarde tarde)))\\n....\\n\\nIn the Spanish corpus, files with an initial letter correspond to\\ndifferent genres (only those from the original 3LB sample):\\n\\n       A       press: articulistas     PRESS: OPINION\\n       E       press: ensayo   PRESS:ESSAY\\n       C       press: suplementosCiencia       PRESS: SCIENCE SUPLEMENT\\n       D       press: prensa deportiva         PRESS: SPORTS\\n       N       press: noticias         PRESS:NEWS\\n       R       press: semanarios       PRESS: WEEKLIES,\\n       T       fiction: narrativa      FICTION: NARRATIVE\\n\\nCESS-ECE corpora should allow both grammar inference for syntactic\\nparsing and the training of Machine Learnig systems for word sense\\ndisambiguation.\\n\\nIf you use these corpora for research, please cite thusly: CESS-Cat\\nproject (M. Antonia MartÃ\\xad, MarionaTaulÃ©, LluÃ\\xads MÃ¡rquez, Manuel\\nBertran (2007) ?CESS-ECE: A Multilingual and Multilevel Annotated\\nCorpus? in http://www.lsi.upc.edu/~mbertran/cess-ece/publications).\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.cess_esp.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('El', 'da0ms0'), ('grupo', 'ncms000'), ...]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.cess_esp.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Me', 'NN'),\n",
       " ('gusta', 'NN'),\n",
       " ('todo', 'NN'),\n",
       " ('como', 'NN'),\n",
       " ('huevos', 'NN'),\n",
       " ('verdes', 'NN'),\n",
       " ('y', 'NN'),\n",
       " ('jamon', 'NN'),\n",
       " ('.', 'NN'),\n",
       " ('No', 'NN'),\n",
       " ('le', 'NN'),\n",
       " ('gustan', 'NN'),\n",
       " ('a', 'NN'),\n",
       " ('Sam', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('yo', 'NN'),\n",
       " ('soy', 'NN'),\n",
       " ('!', 'NN')]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = \"Me gusta todo como huevos verdes y jamon. No le gustan a Sam, yo soy!\"\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "nltk.DefaultTagger(\"NN\").tag(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
